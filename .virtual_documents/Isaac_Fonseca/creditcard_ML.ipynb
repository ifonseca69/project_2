# Imported Libraries
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA, TruncatedSVD
import matplotlib.patches as mpatches
import time

# Classifier Libraries
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
import collections

import xgboost as xgb
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import GridSearchCV
from imblearn.combine import SMOTEENN

from sklearn.metrics import roc_curve, roc_auc_score

# Other Libraries
from imblearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import NearMiss
from imblearn.under_sampling import RandomUnderSampler
from imblearn.metrics import classification_report_imbalanced
from sklearn.metrics import classification_report
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report
from collections import Counter
from sklearn.model_selection import KFold, StratifiedKFold
import warnings
warnings.filterwarnings("ignore")



scaled_train_df = pd.read_csv('../Resources/scaled_train_data.csv')



scaled_train_df.head()


# Get the number of rows for the class
class_distribution = scaled_train_df['Class'].value_counts()
class_percentages = scaled_train_df['Class'].value_counts(normalize=True) * 100

# Combine into a DataFrame for a clearer display
class_summary = pd.DataFrame({'Count': class_distribution, 'Percentage': class_percentages})

# Format the Count and Percentage columns
class_summary['Count'] = class_summary['Count'].apply(lambda x: f"{x:,.3f}")
class_summary['Percentage'] = class_summary['Percentage'].apply(lambda x: f"{x:,.3f}   %")

print(class_summary)


# Assuming scaled_train_df is already defined
X = scaled_train_df.drop('Class', axis=1)
y = scaled_train_df['Class']

# Define SMOTE and RandomUnderSampler
smote = SMOTE(sampling_strategy='minority')
under_sampler = RandomUnderSampler(sampling_strategy='majority')

# Create a pipeline
pipeline = Pipeline(steps=[('smote', smote), ('under', under_sampler)])

# Apply the pipeline to the data
X_resampled, y_resampled = pipeline.fit_resample(X, y)

# Combine resampled data into a DataFrame
resampled_df = pd.DataFrame(X_resampled, columns=X.columns)
resampled_df['Class'] = y_resampled

# Display the new class distribution
class_distribution = resampled_df['Class'].value_counts()
class_percentages = resampled_df['Class'].value_counts(normalize=True) * 100

# Combine into a DataFrame for a clearer display
class_summary = pd.DataFrame({'Count': class_distribution, 'Percentage': class_percentages})

# Format the Count and Percentage columns
class_summary['Count'] = class_summary['Count'].apply(lambda x: f"{x:,.3f}")
class_summary['Percentage'] = class_summary['Percentage'].apply(lambda x: f"{x:,.3f}   %")

print(class_summary)


# Split the data into training and testing sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)





# Logistic Regression
logistic_regression = LogisticRegression()

# Pipelines for over-sampling
over_pipeline_lr = Pipeline(steps=[('smote', smote), ('logistic', logistic_regression)])

# Pipelines for under-sampling
under_pipeline_lr = Pipeline(steps=[('under', under_sampler), ('logistic', logistic_regression)])

# Train and evaluate Logistic Regression with over-sampling
over_pipeline_lr.fit(X_train, y_train)
y_pred_over_lr = over_pipeline_lr.predict(X_val)
print("Logistic Regression with Over-Sampling:\n", classification_report(y_val, y_pred_over_lr))

# Train and evaluate Logistic Regression with under-sampling
under_pipeline_lr.fit(X_train, y_train)
y_pred_under_lr = under_pipeline_lr.predict(X_val)
print("Logistic Regression with Under-Sampling:\n", classification_report(y_val, y_pred_under_lr))







# Assuming scaled_train_df is already defined
X = scaled_train_df.drop('Class', axis=1)
y = scaled_train_df['Class']

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply SMOTEENN to balance the dataset
smote_enn = SMOTEENN(random_state=42)
X_train_resampled, y_train_resampled = smote_enn.fit_resample(X_train, y_train)

# Train XGBoost model
xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', scale_pos_weight=10)
xgb_model.fit(X_train_resampled, y_train_resampled)

# Make predictions
y_pred_xgb = xgb_model.predict(X_val)

# Evaluate the model
accuracy_xgb = accuracy_score(y_val, y_pred_xgb)
report_xgb = classification_report(y_val, y_pred_xgb)

print(f'Accuracy: {accuracy_xgb}')
print(f'Classification Report:\n{report_xgb}')





# Define the parameter grid for GridSearchCV
param_grid = {
    'max_depth': [3, 6, 9, 12],
    'learning_rate': [0.01, 0.1, 0.2],
    'n_estimators': [100, 200, 300],
    'scale_pos_weight': [1, 10, 25],
    'subsample': [0.5, 0.7, 1.0],
    'colsample_bytree': [0.5, 0.7, 1.0]
}

# Create the XGBoost model
xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')

# Create the GridSearchCV object
grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)

# Fit the grid search to the data
grid_search.fit(X_train_resampled, y_train_resampled)

# Get the best parameters and model
best_params = grid_search.best_params_
best_xgb_model = grid_search.best_estimator_

print(f'Best Parameters: {best_params}')

# Evaluate the best model
y_pred_best_xgb = best_xgb_model.predict(X_val)
accuracy_best_xgb = accuracy_score(y_val, y_pred_best_xgb)
report_best_xgb = classification_report(y_val, y_pred_best_xgb)

print(f'Accuracy: {accuracy_best_xgb}')
print(f'Classification Report:\n{report_best_xgb}')



# Train the XGBoost model again if needed
xgb_model.fit(X_train_resampled, y_train_resampled)

# Get feature importances
importances = xgb_model.feature_importances_

# Print or plot the feature importances
for feature, importance in zip(X_train.columns, importances):
    print(f"Feature: {feature}, Importance: {importance}")

# Optionally, plot the feature importances for a visual aid
importances = xgb_model.feature_importances_
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10, 6))
plt.barh(range(len(importances)), importances[indices], align='center')
plt.yticks(range(len(importances)), [X_train.columns[i] for i in indices])
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importances (Descending)')
plt.gca().invert_yaxis()
plt.show()



# Select top N important features (adjust N as needed)
N = 20
important_indices = np.argsort(importances)[-N:]

X_train_important = X_train_resampled.iloc[:, important_indices]
X_val_important = X_val.iloc[:, important_indices]

# Train the XGBoost model with the important features
xgb_model_important = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', scale_pos_weight=10)
xgb_model_important.fit(X_train_important, y_train_resampled)

# Make predictions
y_pred_important = xgb_model_important.predict(X_val_important)

# Evaluate the model
accuracy_important = accuracy_score(y_val, y_pred_important)
report_important = classification_report(y_val, y_pred_important)

print(f'Accuracy: {accuracy_important}')
print(f'Classification Report:\n{report_important}')



import pickle
# Save the model to a file
with open('best_xgb_model.pkl', 'wb') as file:
    pickle.dump(best_xgb_model, file)


# Later use X_test 
# Load the model from the file
with open('best_xgb_model.pkl', 'rb') as file:
    loaded_model = pickle.load(file)
# Use the loaded model to make predictions
predictions = loaded_model.predict(X_train)


scaled_test_df = pd.read_csv('../Resources/scaled_test_data.csv')


# Drop the Class for the scaled_test data 
X_test = scaled_test_df.drop('Class', axis=1)
y_test = scaled_test_df['Class']


# Later use X_test 
# Load the model from the file
with open('best_xgb_model.pkl', 'rb') as file:
    loaded_model = pickle.load(file)
# Use the loaded model to make predictions
predictions = loaded_model.predict(X_test)


ssr = ((predictions - y_test)**2).sum()
ssr


# Production with the best model
with open('best_xgb_model.pkl', 'rb') as file:
    loaded_xgb_model = pickle.load(file)
y_pred = loaded_xgb_model.predict(X_test)
accuracy_production_xgb = accuracy_score(y_test, y_pred)
report_production_xgb = classification_report(y_test, y_pred)

print(f'Accuracy: {accuracy_production_xgb}')
print(f'Classification Report:\n{report_production_xgb}')




# Get predicted probabilities
y_prob = loaded_xgb_model.predict_proba(X_test)[:, 1]

# Compute ROC curve and AUC score
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = roc_auc_score(y_test, y_prob)

# Plot ROC curve
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()




