# Imported Libraries
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.patches as mpatches
import time

# Classifier Libraries
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
import collections

import xgboost as xgb
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import GridSearchCV
from imblearn.combine import SMOTEENN
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint
from sklearn.metrics import roc_curve, roc_auc_score

# Other Libraries
from imblearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import NearMiss
from imblearn.under_sampling import RandomUnderSampler
from imblearn.metrics import classification_report_imbalanced
from sklearn.metrics import classification_report
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report
from collections import Counter
from sklearn.model_selection import KFold, StratifiedKFold
import warnings
warnings.filterwarnings("ignore")



scaled_train_df = pd.read_csv('../Resources/scaled_train_data.csv')



scaled_train_df.head()


# Get the number of rows for the class
class_distribution = scaled_train_df['Class'].value_counts()
class_percentages = scaled_train_df['Class'].value_counts(normalize=True) * 100

# Combine into a DataFrame for a clearer display
class_summary = pd.DataFrame({'Count': class_distribution, 'Percentage': class_percentages})

# Format the Count and Percentage columns
class_summary['Count'] = class_summary['Count'].apply(lambda x: f"{x:,.3f}")
class_summary['Percentage'] = class_summary['Percentage'].apply(lambda x: f"{x:,.3f}   %")

print(class_summary)


# Assuming scaled_train_df is already defined
X = scaled_train_df.drop('Class', axis=1)
y = scaled_train_df['Class']

# Define SMOTE and RandomUnderSampler
smote = SMOTE(sampling_strategy='minority')
under_sampler = RandomUnderSampler(sampling_strategy='majority')

# Create a pipeline
pipeline = Pipeline(steps=[('smote', smote), ('under', under_sampler)])

# Apply the pipeline to the data
X_resampled, y_resampled = pipeline.fit_resample(X, y)

# Combine resampled data into a DataFrame
resampled_df = pd.DataFrame(X_resampled, columns=X.columns)
resampled_df['Class'] = y_resampled

# Display the new class distribution
class_distribution = resampled_df['Class'].value_counts()
class_percentages = resampled_df['Class'].value_counts(normalize=True) * 100

# Combine into a DataFrame for a clearer display
class_summary = pd.DataFrame({'Count': class_distribution, 'Percentage': class_percentages})

# Format the Count and Percentage columns
class_summary['Count'] = class_summary['Count'].apply(lambda x: f"{x:,.3f}")
class_summary['Percentage'] = class_summary['Percentage'].apply(lambda x: f"{x:,.3f}   %")

print(class_summary)


# Split the data into training and testing sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)





# Logistic Regression
logistic_regression = LogisticRegression()

# Pipelines for over-sampling
over_pipeline_lr = Pipeline(steps=[('smote', smote), ('logistic', logistic_regression)])

# Pipelines for under-sampling
under_pipeline_lr = Pipeline(steps=[('under', under_sampler), ('logistic', logistic_regression)])

# Train and evaluate Logistic Regression with over-sampling
over_pipeline_lr.fit(X_train, y_train)
y_pred_over_lr = over_pipeline_lr.predict(X_val)
print("Logistic Regression with Over-Sampling:\n", classification_report(y_val, y_pred_over_lr))

# Train and evaluate Logistic Regression with under-sampling
under_pipeline_lr.fit(X_train, y_train)
y_pred_under_lr = under_pipeline_lr.predict(X_val)
print("Logistic Regression with Under-Sampling:\n", classification_report(y_val, y_pred_under_lr))







# Assuming scaled_train_df is already defined
X = scaled_train_df.drop('Class', axis=1)
y = scaled_train_df['Class']

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply SMOTEENN to balance the dataset
smote_enn = SMOTEENN(random_state=42)
X_train_resampled, y_train_resampled = smote_enn.fit_resample(X_train, y_train)

# Train XGBoost model
xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', scale_pos_weight=10)
xgb_model.fit(X_train_resampled, y_train_resampled)

# Make predictions
y_pred_xgb = xgb_model.predict(X_val)

# Evaluate the model
accuracy_xgb = accuracy_score(y_val, y_pred_xgb)
report_xgb = classification_report(y_val, y_pred_xgb)

print(f'Accuracy: {accuracy_xgb}')
print(f'Classification Report:\n{report_xgb}')


# Original class distribution
class_counts_original = y_train.value_counts()

# Resampled class distribution
class_counts_resampled = y_train_resampled.value_counts()

# Create a DataFrame for visualization
class_distribution_original_df = pd.DataFrame({
    'Class': ['Non-Fraud', 'Fraud'],
    'Count': class_counts_original.values
})

class_distribution_resampled_df = pd.DataFrame({
    'Class': ['Non-Fraud', 'Fraud'],
    'Count': class_counts_resampled.values
})

# Plot the original class distribution
plt.figure(figsize=(10, 6))
sns.barplot(x='Class', y='Count', data=class_distribution_original_df, palette='Blues')
plt.xlabel('Class')
plt.ylabel('Number of Samples')
plt.title('Original Class Distribution')
plt.grid(True)
plt.savefig('../output/original_class_distribution.png', format='png')
plt.show()

# Plot the resampled class distribution
plt.figure(figsize=(10, 6))
sns.barplot(x='Class', y='Count', data=class_distribution_resampled_df, palette='Reds')
plt.xlabel('Class')
plt.ylabel('Number of Samples')
plt.title('Resampled Class Distribution After Applying SMOTEENN')
plt.grid(True)
plt.savefig('../output/resampled_class_distribution_smoteenn.png', format='png')
plt.show()





# Define the parameter grid for GridSearchCV
param_grid = {
    'max_depth': [3, 6, 9, 12],
    'learning_rate': [0.01, 0.1, 0.2],
    'n_estimators': [100, 200, 300],
    'scale_pos_weight': [1, 10, 25],
    'subsample': [0.5, 0.7, 1.0],
    'colsample_bytree': [0.5, 0.7, 1.0]
}

xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')

# Create the GridSearchCV object
grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)

# Fit the grid search to the data
grid_search.fit(X_train_resampled, y_train_resampled)

# Get the best parameters and model
best_params = grid_search.best_params_
best_xgb_model = grid_search.best_estimator_

print(f'Best Parameters: {best_params}')

# Evaluate the best model
y_pred_best_xgb = best_xgb_model.predict(X_val)
accuracy_best_xgb = accuracy_score(y_val, y_pred_best_xgb)
report_best_xgb = classification_report(y_val, y_pred_best_xgb)

print(f'Accuracy: {accuracy_best_xgb}')
print(f'Classification Report:\n{report_best_xgb}')






# Save the accuracy and classification report to a text file
with open('../output/model_evaluation.txt', 'w') as f:
    f.write(f'Best Parameters: {best_params}\n')
    f.write(f'Accuracy: {accuracy_best_xgb}\n') 
    f.write(f'Classification Report:\n{report_best_xgb}\n')



# Train the XGBoost model again if needed
xgb_model.fit(X_train_resampled, y_train_resampled)

# Get feature importances
importances = xgb_model.feature_importances_

# Print or plot the feature importances
for feature, importance in zip(X_train.columns, importances):
    print(f"Feature: {feature}, Importance: {importance}")

# Optionally, plot the feature importances for a visual aid
importances = xgb_model.feature_importances_
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10, 6))
plt.barh(range(len(importances)), importances[indices], align='center')
plt.yticks(range(len(importances)), [X_train.columns[i] for i in indices])
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importances (Descending)')
plt.gca().invert_yaxis()
plt.savefig('../output/feature_importances.png', format='png')
plt.show()



import pickle
# Save the model to a file
with open('best_xgb_model.pkl', 'wb') as file:
    pickle.dump(best_xgb_model, file)


# Later use X_test 
# Load the model from the file
with open('best_xgb_model.pkl', 'rb') as file:
    loaded_model = pickle.load(file)
# Use the loaded model to make predictions
predictions = loaded_model.predict(X_train)


scaled_test_df = pd.read_csv('../Resources/scaled_test_data.csv')


# Drop the Class for the scaled_test data 
X_test = scaled_test_df.drop('Class', axis=1)
y_test = scaled_test_df['Class']


# Later use X_test 
# Load the model from the file
with open('best_xgb_model.pkl', 'rb') as file:
    loaded_model = pickle.load(file)
# Use the loaded model to make predictions
predictions = loaded_model.predict(X_test)


ssr = ((predictions - y_test)**2).sum()
ssr


# Production with the best model
with open('best_xgb_model.pkl', 'rb') as file:
    loaded_xgb_model = pickle.load(file)
y_pred = loaded_xgb_model.predict(X_test)
accuracy_production_xgb = accuracy_score(y_test, y_pred)
report_production_xgb = classification_report(y_test, y_pred)

print(f'Accuracy: {accuracy_production_xgb}')
print(f'Classification Report:\n{report_production_xgb}')



# Save the accuracy and classification report to a text file
with open('../output/production_xgb_model_evaluation.txt', 'w') as f:
    f.write(f'Accuracy: {accuracy_production_xgb}\n') 
    f.write(f'Classification Report:\n{report_production_xgb}\n')




# Get predicted probabilities
y_prob = loaded_xgb_model.predict_proba(X_test)[:, 1]

# Compute ROC curve and AUC score
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = roc_auc_score(y_test, y_prob)

# Plot ROC curve
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.savefig('../output/roc_curve.png', format='png')
plt.show()



#Stacking multiple training model

from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier

base_models = [
    ('dt', DecisionTreeClassifier()),
    ('knn', KNeighborsClassifier())
]




#Train Meta-Model

meta_model = LogisticRegression()
stacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_model)
stacking_model.fit(X_train_resampled, y_train_resampled)



# Evaluate
y_pred_stack = stacking_model.predict(X_val)
accuracy_stack = accuracy_score(y_val, y_pred_stack)
report_stack = classification_report(y_val, y_pred_stack)

print(f'Accuracy: {accuracy_stack}')
print(f'Classification Report:\n{report_stack}')



from sklearn.ensemble import BaggingClassifier

bagging_model = BaggingClassifier(estimator=xgb_model, n_estimators=10, random_state=42)
bagging_model.fit(X_train_resampled, y_train_resampled)




# Evaluate
y_pred_bag = bagging_model.predict(X_val)
accuracy_bag = accuracy_score(y_val, y_pred_bag)
report_bag = classification_report(y_val, y_pred_bag)

print(f'Accuracy: {accuracy_bag}')
print(f'Classification Report:\n{report_bag}')



# Train Boosting Model

from xgboost import XGBClassifier

boosting_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', scale_pos_weight=10)
boosting_model.fit(X_train_resampled, y_train_resampled)



# Evaluate
y_pred_boost = boosting_model.predict(X_val)
accuracy_boost = accuracy_score(y_val, y_pred_boost)
report_boost = classification_report(y_val, y_pred_boost)

print(f'Accuracy: {accuracy_boost}')
print(f'Classification Report:\n{report_boost}')




