# Imported Libraries
import pandas as pd
import numpy as np # linear algebra
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")


df = pd.read_csv('../Resources/creditcard.csv')
df.tail()


# Get a concise summary of the DataFrame, including the data types and non-null values
df_info = df.info()

# Get the shape of the DataFrame (number of rows and columns)
df_shape = df.shape
print("Shape of the DataFrame:", df_shape)


df.describe()


# Create the count plot to visualize the imbalance data with a percentages of fraud and no fraud
ax = sns.countplot(x='Class', data=df, palette={'0': 'blue', '1': 'red'})


# Calculate percentages
total = len(df)
for p in ax.patches:
    percentage = '{:.2f}%'.format(100 * p.get_height() / total)
    x = p.get_x() + p.get_width() / 2
    y = p.get_height()
    ax.annotate(percentage, (x, y), ha='center', va='bottom')

# Add the title
plt.title('Class Distributions \n (0: No Fraud || 1: Fraud)', fontsize=14)

# Show the plot
plt.show()


# Scatter Plot by the minute for the transactions that occurred in two days, 
# where we have 492 frauds out of 284,807 transactions. 

# Convert Time seconds to minutes
df['Minute'] = df['Time'] // 60

# Plot the transactions with larger markers for fraud
plt.figure(figsize=(14, 7))

# Plot Non-Fraud with default size
sns.scatterplot(data=df[df['Class'] == 0], x='Minute', y='Amount', color='blue', marker='o', alpha=0.6, label='Non-Fraud (Blue Circle)')

# Plot Fraud with larger size
sns.scatterplot(data=df[df['Class'] == 1], x='Minute', y='Amount', color='red', marker='X', s=150, alpha=0.6, label='Fraud (Red X)')

plt.xlabel('Time 48 hours in (Minutes)')
plt.ylabel('Transaction Amount')
plt.title('Transactions by Time 48 hours in (Minutes)')

# Update the legend
handles, labels = plt.gca().get_legend_handles_labels()
plt.legend(handles=handles, labels=labels, title='Class', loc='upper right')
plt.grid(True)  # Add grid lines
plt.show()



# Scatter Plot by the minute for the transactions that occurred in two days, 
# where we have 492 frauds out of 284,807 transactions. 

# Extend to 48 hours (two days)
df['Hour'] = (df['Time'] // 3600) % 48

# Plot the transactions with larger markers for fraud
plt.figure(figsize=(14, 7))

# Plot Non-Fraud with default size
sns.scatterplot(data=df[df['Class'] == 0], x='Hour', y='Amount', color='blue', marker='o', alpha=0.6, label='Non-Fraud (Blue Circle)')

# Plot Fraud with larger size
sns.scatterplot(data=df[df['Class'] == 1], x='Hour', y='Amount', color='red', marker='X', s=150, alpha=0.6, label='Fraud (Red X)')

plt.xlabel('Hour of the Day')
plt.ylabel('Transaction Amount')
plt.title('Transactions by Hour for Two Days')

# Update the legend
handles, labels = plt.gca().get_legend_handles_labels()
plt.legend(handles=handles, labels=labels, title='Class', loc='upper right')
plt.grid(True)  # Add grid lines
plt.show()


# Class 0 Describe Non Fraud Vs Class 1 Fraud
split = df[['Amount', 'Class']].copy()
class_0 = split.loc[split['Class'] == 0]['Amount']
class_1 = split.loc[split['Class'] == 1]['Amount']

print("Class 0 (Non-Fraud) Summary:")
print(class_0.describe())

print("\nClass 1 (Fraud) Summary:")
print(class_1.describe())




# Create a figure with two subplots
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Quartile plot for Class 0 (Non-Fraud)
sns.boxplot(y=class_0, ax=axes[0], color='blue')
axes[0].set_title('Class 0 (Non-Fraud)')
axes[0].set_ylabel('Transaction Amount')

# Quartile plot for Class 1 (Fraud)
sns.boxplot(y=class_1, ax=axes[1], color='red')
axes[1].set_title('Class 1 (Fraud)')
axes[1].set_ylabel('Transaction Amount')

plt.tight_layout()
plt.show()



# The kde "Kernel Density Plot" it shows the probability density function of the data, to understandad the feautures distribution of the data points

t0 = df[df['Class'] == 0]
t1 = df[df['Class'] == 1]

# var is a list of features in the dataframe dfs
var = df.columns  # or specify the list of features you want to plot

# Adjust the number of rows and columns based on the number of features
num_features = len(var)
num_cols = 4
num_rows = (num_features + num_cols - 1) // num_cols

plt.figure(figsize=(15, num_rows * 4))

for i, feature in enumerate(var, 1):
    plt.subplot(num_rows, num_cols, i)
    sns.kdeplot(t0[feature], bw=0.5, label="Class = 0")
    sns.kdeplot(t1[feature], bw=0.5, label="Class = 1")
    plt.legend()

plt.tight_layout()
plt.savefig('kde_plots.png')
plt.show()






# Calculate 70% of the maximum Time value for train split
train_threshold = df['Time'].max() * 0.7

# Create a new column to indicate the split
df['Split'] = ['Train' if x <= train_threshold else 'Test' for x in df['Time']]

# Verify the split
print(df['Split'].value_counts())

# Plot train and test data
plt.figure(figsize=(14, 7))

# Plot Train data
sns.scatterplot(data=df[df['Split'] == 'Train'], x='Time', y='Amount', color='blue', marker='o', alpha=0.6, label='Train Data (Green Circle)')

# Plot Test data
sns.scatterplot(data=df[df['Split'] == 'Test'], x='Time', y='Amount', color='orange', marker='X', s=150, alpha=0.6, label='Test Data (Orange X)')

plt.xlabel('Time (Seconds)')
plt.ylabel('Transaction Amount')
plt.title('Transactions by Time (Seconds) - Train and Test Split')

# Update the legend
handles, labels = plt.gca().get_legend_handles_labels()
plt.legend(handles=handles, labels=labels, title='Dataset Split', loc='upper right')
plt.grid(True)  # Add grid lines
plt.show()


# Calculate 70% of the maximum Time value for train split
train_threshold = df['Time'].max() * 0.7

# Create a new column to indicate the split
df['Split'] = ['Train' if x <= train_threshold else 'Test' for x in df['Time']]

# Separate the data
train_data = df[df['Split'] == 'Train']
test_data = df[df['Split'] == 'Test']

# drop the column 'Split'

train_data = train_data.drop(columns=['Split'])
test_data = test_data.drop(columns=['Split'])

# Save to CSV files
train_data.to_csv('../Resources/train_data.csv', index=False)
test_data.to_csv('../Resources/test_data.csv', index=False)

print("Train and Test data have been saved to 'train_data.csv' and 'test_data.csv'.")


# Get the unique values in the 'class' column
unique_classes = df['Class'].unique()
print("Unique values in 'Class' column:", unique_classes)


# All the V futures have already been scaled except for the Amount and Time, which will need to be scaled

from sklearn.preprocessing import StandardScaler, RobustScaler

# RobustScaler is less prone to outliers.

std_scaler = StandardScaler()
rob_scaler = RobustScaler()

df['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))
df['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))

df.drop(['Time','Amount'], axis=1, inplace=True)


scaled_amount = df['scaled_amount']
scaled_time = df['scaled_time']

df.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)
df.insert(0, 'scaled_amount', scaled_amount)
df.insert(1, 'scaled_time', scaled_time)

# Amount and Time are Scaled!

df.head()


train_df = pd.read_csv('../Resources/train_data.csv')


train_df.head()


# Create the count plot to visualize the imbalance data with percentages of fraud and no fraud
ax = sns.countplot(x='Class', data=train_df, palette={'0': 'blue', '1': 'red'})

# Calculate percentages and add annotations
total = len(train_df)
for p in ax.patches:
    count = p.get_height()
    percentage = '{:.2f}%'.format(100 * count / total)
    x = p.get_x() + p.get_width() / 2
    y = p.get_height()
    ax.annotate(f'{count}\n({percentage})', (x, y), ha='center', va='bottom')

# Add the title
plt.title('Train Data Class Distributions   \n      (0: No Fraud || 1: Fraud)', fontsize=14)

# Show the plot
plt.show()


from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Assuming test_df already contains the correct 'Class' column
X = train_df.drop(columns=['Class'])
y = train_df['Class']

# Ensure all data is numeric and handle NaNs
X = X.apply(pd.to_numeric, errors='coerce')
y = pd.to_numeric(y, errors='coerce')

# Drop rows with NaN values
df_cleaned = pd.concat([X, y], axis=1).dropna()
X = df_cleaned.drop(columns=['Class'])
y = df_cleaned['Class']

# Use train_test_split to split the data
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)

# Apply SMOTE to balance the dataset
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Apply Logistic Regression
model = LogisticRegression()
model.fit(X_train_smote, y_train_smote)

# Make predictions
y_pred = model.predict(X_val)

# Evaluate the model
accuracy = accuracy_score(y_val, y_pred)
report = classification_report(y_val, y_pred)

print(f'Accuracy: {accuracy}')
print(f'Classification Report:\n{report}')








