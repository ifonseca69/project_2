# Imported Libraries
import pandas as pd
import numpy as np # linear algebra
import matplotlib.pyplot as plt
import seaborn as sns



df = pd.read_csv('../Resources/creditcard.csv')
df.tail()


# Get a concise summary of the DataFrame, including the data types and non-null values
df_info = df.info()

# Get the shape of the DataFrame (number of rows and columns)
df_shape = df.shape
print("Shape of the DataFrame:", df_shape)


df.describe()


# Create the count plot to visualize the imbalance data with a percentages of fraud and no fraud
ax = sns.countplot(x='Class', data=df, palette={0: 'blue', 1: 'red'})

# Calculate percentages
total = len(df)
for p in ax.patches:
    percentage = '{:.2f}%'.format(100 * p.get_height() / total)
    x = p.get_x() + p.get_width() / 2
    y = p.get_height()
    ax.annotate(percentage, (x, y), ha='center', va='bottom')

# Add the title
plt.title('Class Distributions \n (0: No Fraud || 1: Fraud)', fontsize=14)

# Show the plot
plt.show()


# Scatter Plot by the minute for the transactions that occurred in two days, 
# where we have 492 frauds out of 284,807 transactions. 

# Convert Time seconds to minutes
df['Minute'] = df['Time'] // 60

# Plot the transactions with larger markers for fraud
plt.figure(figsize=(14, 7))

# Plot Non-Fraud with default size
sns.scatterplot(data=df[df['Class'] == 0], x='Minute', y='Amount', color='blue', marker='o', alpha=0.6, label='Non-Fraud (Blue Circle)')

# Plot Fraud with larger size
sns.scatterplot(data=df[df['Class'] == 1], x='Minute', y='Amount', color='red', marker='X', s=150, alpha=0.6, label='Fraud (Red X)')

plt.xlabel('Time 48 hours in (Minutes)')
plt.ylabel('Transaction Amount')
plt.title('Transactions by Time 48 hours in (Minutes)')

# Update the legend
handles, labels = plt.gca().get_legend_handles_labels()
plt.legend(handles=handles, labels=labels, title='Class', loc='upper right')
plt.grid(True)  # Add grid lines
plt.show()



# Scatter Plot by the minute for the transactions that occurred in two days, 
# where we have 492 frauds out of 284,807 transactions. 

# Extend to 48 hours (two days)
df['Hour'] = (df['Time'] // 3600) % 48

# Plot the transactions with larger markers for fraud
plt.figure(figsize=(14, 7))

# Plot Non-Fraud with default size
sns.scatterplot(data=df[df['Class'] == 0], x='Hour', y='Amount', color='blue', marker='o', alpha=0.6, label='Non-Fraud (Blue Circle)')

# Plot Fraud with larger size
sns.scatterplot(data=df[df['Class'] == 1], x='Hour', y='Amount', color='red', marker='X', s=150, alpha=0.6, label='Fraud (Red X)')

plt.xlabel('Hour of the Day')
plt.ylabel('Transaction Amount')
plt.title('Transactions by Hour for Two Days')

# Update the legend
handles, labels = plt.gca().get_legend_handles_labels()
plt.legend(handles=handles, labels=labels, title='Class', loc='upper right')
plt.grid(True)  # Add grid lines
plt.show()


# Calculate 70% of the maximum Time value for train split
train_threshold = df['Time'].max() * 0.7

# Create a new column to indicate the split
df['Split'] = ['Train' if x <= train_threshold else 'Test' for x in df['Time']]

# Verify the split
print(df['Split'].value_counts())

# Plot train and test data
plt.figure(figsize=(14, 7))

# Plot Train data
sns.scatterplot(data=df[df['Split'] == 'Train'], x='Time', y='Amount', color='blue', marker='o', alpha=0.6, label='Train Data (Green Circle)')

# Plot Test data
sns.scatterplot(data=df[df['Split'] == 'Test'], x='Time', y='Amount', color='orange', marker='X', s=150, alpha=0.6, label='Test Data (Orange X)')

plt.xlabel('Time (Seconds)')
plt.ylabel('Transaction Amount')
plt.title('Transactions by Time (Seconds) - Train and Test Split')

# Update the legend
handles, labels = plt.gca().get_legend_handles_labels()
plt.legend(handles=handles, labels=labels, title='Dataset Split', loc='upper right')
plt.grid(True)  # Add grid lines
plt.show()


# Calculate 70% of the maximum Time value for train split
train_threshold = df['Time'].max() * 0.7

# Create a new column to indicate the split
df['Split'] = ['Train' if x <= train_threshold else 'Test' for x in df['Time']]

# Separate the data
train_data = df[df['Split'] == 'Train']
test_data = df[df['Split'] == 'Test']

# drop the column 'Split'

train_data = train_data.drop(columns=['Split'])
test_data = test_data.drop(columns=['Split'])

# Save to CSV files
train_data.to_csv('../Resources/train_data.csv', index=False)
test_data.to_csv('../Resources/test_data.csv', index=False)

print("Train and Test data have been saved to 'train_data.csv' and 'test_data.csv'.")


# Get the unique values in the 'class' column
unique_classes = df['Class'].unique()
print("Unique values in 'Class' column:", unique_classes)


# All the V futures have already been scaled except for the Amount and Time, which will need to be scaled

from sklearn.preprocessing import StandardScaler, RobustScaler

# RobustScaler is less prone to outliers.

std_scaler = StandardScaler()
rob_scaler = RobustScaler()

df['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))
df['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))

df.drop(['Time','Amount'], axis=1, inplace=True)


scaled_amount = df['scaled_amount']
scaled_time = df['scaled_time']

df.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)
df.insert(0, 'scaled_amount', scaled_amount)
df.insert(1, 'scaled_time', scaled_time)

# Amount and Time are Scaled!

df.head().


train_df = pd.read_csv('../Resources/train_data.csv')


train_df.head()


# Create the count plot to visualize the imbalance data with percentages of fraud and no fraud
ax = sns.countplot(x='Class', data=train_df, palette={0: 'blue', 1: 'red'})

# Calculate percentages and add annotations
total = len(train_df)
for p in ax.patches:
    count = p.get_height()
    percentage = '{:.2f}%'.format(100 * count / total)
    x = p.get_x() + p.get_width() / 2
    y = p.get_height()
    ax.annotate(f'{count}\n({percentage})', (x, y), ha='center', va='bottom')

# Add the title
plt.title('Train Data Class Distributions   \n      (0: No Fraud || 1: Fraud)', fontsize=14)

# Show the plot
plt.show()


from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Assuming test_df already contains the correct 'Class' column
X = test_df.drop(columns=['Class'])
y = test_df['Class']

# Ensure all data is numeric and handle NaNs
X = X.apply(pd.to_numeric, errors='coerce')
y = pd.to_numeric(y, errors='coerce')

# Drop rows with NaN values
df_cleaned = pd.concat([X, y], axis=1).dropna()
X = df_cleaned.drop(columns=['Class'])
y = df_cleaned['Class']

# Use train_test_split to split the data
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)

# Apply SMOTE to balance the dataset
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Apply Logistic Regression
model = LogisticRegression()
model.fit(X_train_smote, y_train_smote)

# Make predictions
y_pred = model.predict(X_val)

# Evaluate the model
accuracy = accuracy_score(y_val, y_pred)
report = classification_report(y_val, y_pred)

print(f'Accuracy: {accuracy}')
print(f'Classification Report:\n{report}')




from imblearn.under_sampling import RandomUnderSampler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Assuming test_df already contains the correct 'Class' column
X = test_df.drop(columns=['Class'])
y = test_df['Class']

# Ensure all data is numeric and handle NaNs
X = X.apply(pd.to_numeric, errors='coerce')
y = pd.to_numeric(y, errors='coerce')

# Drop rows with NaN values
df_cleaned = pd.concat([X, y], axis=1).dropna()
X = df_cleaned.drop(columns=['Class'])
y = df_cleaned['Class']

# Use train_test_split to split the data
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)

# Apply RandomUnderSampler to balance the dataset
rus = RandomUnderSampler(random_state=42)
X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)

# Apply Logistic Regression
model = LogisticRegression()
model.fit(X_train_rus, y_train_rus)

# Make predictions
y_pred = model.predict(X_val)

# Evaluate the model
accuracy = accuracy_score(y_val, y_pred)
report = classification_report(y_val, y_pred)

print(f'Accuracy: {accuracy}')
print(f'Classification Report:\n{report}')



from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Apply Random Forest Classifier
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train_rus, y_train_rus)

# Make predictions
y_pred_rf = rf_model.predict(X_val)

# Evaluate the model
accuracy_rf = accuracy_score(y_val, y_pred_rf)
report_rf = classification_report(y_val, y_pred_rf)

print(f'Accuracy: {accuracy_rf}')
print(f'Classification Report:\n{report_rf}')


from sklearn.ensemble import RandomForestClassifier

# Apply Random Forest Classifier with class weights
rf_model = RandomForestClassifier(random_state=42, class_weight={0: 1, 1: 10})  # Adjust the weight for fraud class
rf_model.fit(X_train_rus, y_train_rus)

# Make predictions
y_pred_rf = rf_model.predict(X_val)

# Evaluate the model
accuracy_rf = accuracy_score(y_val, y_pred_rf)
report_rf = classification_report(y_val, y_pred_rf)

print(f'Accuracy: {accuracy_rf}')
print(f'Classification Report:\n{report_rf}')


from sklearn.model_selection import GridSearchCV

# Define the parameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

# Create a Random Forest model
rf = RandomForestClassifier(random_state=42, class_weight={0: 1, 1: 10})

# Create the GridSearchCV object
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)

# Fit the grid search to the data
grid_search.fit(X_train_rus, y_train_rus)

# Get the best parameters and model
best_params = grid_search.best_params_
best_rf = grid_search.best_estimator_

print(f'Best Parameters: {best_params}')

# Evaluate the best model
y_pred_best_rf = best_rf.predict(X_val)
accuracy_best_rf = accuracy_score(y_val, y_pred_best_rf)
report_best_rf = classification_report(y_val, y_pred_best_rf)

print(f'Accuracy: {accuracy_best_rf}')
print(f'Classification Report:\n{report_best_rf}')



from imblearn.combine import SMOTETomek
from imblearn.under_sampling import RandomUnderSampler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split

# Assuming test_df already contains the correct 'Class' column
X = train_df.drop(columns=['Class'])
y = train_df['Class']

# Ensure all data is numeric and handle NaNs
X = X.apply(pd.to_numeric, errors='coerce')
y = pd.to_numeric(y, errors='coerce')

# Drop rows with NaN values
df_cleaned = pd.concat([X, y], axis=1).dropna()
X = df_cleaned.drop(columns=['Class'])
y = df_cleaned['Class']

# Use train_test_split to split the data
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)

# Combine SMOTE and RandomUnderSampler
smote_tomek = SMOTETomek(random_state=42)
X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train, y_train)

# Apply Random Forest Classifier with the best parameters
rf_model = RandomForestClassifier(random_state=42, 
                                  n_estimators=300, 
                                  max_depth=10, 
                                  max_features='auto', 
                                  min_samples_leaf=4, 
                                  min_samples_split=2, 
                                  bootstrap=True, 
                                  class_weight={0: 1, 1: 10})

rf_model.fit(X_train_resampled, y_train_resampled)

# Make predictions
y_pred_rf = rf_model.predict(X_val)

# Evaluate the model
accuracy_rf = accuracy_score(y_val, y_pred_rf)
report_rf = classification_report(y_val, y_pred_rf)

print(f'Accuracy: {accuracy_rf}')
print(f'Classification Report:\n{report_rf}')




