# Imports
import pandas as pd
import numpy as np # linear algebra
import matplotlib.pyplot as plt
import seaborn as sns
from imblearn.over_sampling import SMOTE
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import tree

# Needed for decision tree visualization
import pydotplus
from IPython.display import Image


train_df = pd.read_csv('../Resources/scaled_train_data.csv')
train_df.head()


X = train_df.drop(columns=['Class'])
y = train_df['Class']


# Ensuring all data is numeric 
X = X.apply(pd.to_numeric, errors='coerce')
y = pd.to_numeric(y, errors='coerce')


# Drop rows with NaN values
df_cleaned = pd.concat([X, y], axis=1).dropna()
X = df_cleaned.drop(columns=['Class'])
y = df_cleaned['Class']


# Using train_test_split to split the data
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)


scaled_amount = train_df['scaled_amount']
scaled_time = train_df['scaled_time']
train_df.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)
train_df.insert(28, 'scaled_amount', scaled_amount)
train_df.insert(29, 'scaled_time', scaled_time)
train_df.head()


# Apply SMOTE to balance the dataset
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)


# Create the decision tree classifier instance
model = tree.DecisionTreeClassifier()
# Fit the model
model = model.fit(X_train_smote, y_train_smote)


# Making predictions
y_pred = model.predict(X_val)


# Calculate the accuracy score
acc_score = accuracy_score(y_val, y_pred)

print(f"Accuracy Score : {acc_score}")


from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

accuracy = accuracy_score(y_val, y_pred)
precision = precision_score(y_val, y_pred)
recall = recall_score(y_val, y_pred)
f1 = f1_score(y_val, y_pred)

print(f'Accuracy: {accuracy}')
print(f'Precision: {precision}')
print(f'Recall: {recall}')
print(f'F1 Score: {f1}')


from sklearn.metrics import classification_report

print(classification_report(y_val, y_pred))


# Create the parameter object for the randomized search estimator.
# Try adjusting n_neighbors with values of 1 through 19. 
# Adjust leaf_size by using a range from 1 to 500.
# Include both uniform and distance options for weights.
param_grid = {
    'max_depth': np.arange(1,40,2),
    'min_samples_split': [2, 10, 20],
    'min_samples_leaf': [1, 5, 10],
    'max_features': ['auto', 'sqrt', 'log2'],
    'criterion': ['gini', 'entropy']
}
param_grid


# Create the randomized search estimator
from sklearn.model_selection import RandomizedSearchCV
random_clf = RandomizedSearchCV(model, param_grid, random_state=0, verbose=3)


# Fit the model by using the randomized search estimator.
random_clf.fit(X_train, y_train)


# List the best parameters for this dataset
print(random_clf.best_params_)


# Make predictions with the hypertuned model
random_tuned_pred = random_clf.predict(X_train)


from sklearn.metrics import classification_report

print(classification_report(y_train, random_tuned_pred))


from sklearn.metrics import confusion_matrix
#Get the confusion matrix
cf_matrix = confusion_matrix(y_val, y_pred)
print(cf_matrix)

import seaborn as sns
sns.heatmap(cf_matrix, annot=True)


# Create DOT data
dot_data = tree.export_graphviz(
    model, out_file=None, feature_names=X.columns, class_names=["0", "1"], filled=True)
# Draw graph
graph = pydotplus.graph_from_dot_data(dot_data)
# Show graph
Image(graph.create_png())


# Save the tree as PDF
file_path = "decision_tree.pdf"
graph.write_pdf(file_path)

# Save the tree as PNG
file_path = "decision_tree.png"
graph.write_png(file_path)


import pickle
# Save the model to a file
with open('model.pkl', 'wb') as file: 
    pickle.dump(model, file)
# Load the model from the file
with open('model.pkl', 'rb') as file:
    model = pickle.load(file)
# Use the loaded model to make predictions
predictions = model.predict(X_train) 



